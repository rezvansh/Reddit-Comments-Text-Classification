{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Comment_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "alOGpP1TMtiz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import csv\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df_train = pd.read_csv(\"/content/drive/My Drive/reddit/reddit_train.csv\") #you can change this to your files' path\n",
        "df_train = shuffle(df_train) # we shuffe the training data to improve the accuracy\n",
        "\n",
        "df_test = pd.read_csv(\"/content/drive/My Drive/reddit/reddit_test.csv\") #you can change this to your files' path\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu1qvOGJ6LTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from nltk.stem import LancasterStemmer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        " #######****************************************************Models********* ********************************  \n",
        "  \n",
        "class models:\n",
        "  \n",
        "  def __init__(self, x_train, y_train, x_test  ):\n",
        "    self.x_train = x_train\n",
        "    self.y_train = y_train\n",
        "    self.x_test = x_test\n",
        "    \n",
        "    \n",
        "  def preprocessing_data_xtrain(self):\n",
        "    stop_words =['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
        "                'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
        "                'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
        "                \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
        "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
        "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
        "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
        "                'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "                'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "                'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
        "                'with', 'about', 'against', 'between', 'into', 'through', \n",
        "                'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
        "                'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
        "                'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
        "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
        "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
        "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't',\n",
        "                'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n",
        "                'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
        "                \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',\n",
        "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
        "                \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
        "                'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", \n",
        "                'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
        "                'won', \"won't\", 'wouldn', \"wouldn't\",\"i've\",\"they've\",\"would\",\n",
        "               \"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\"]\n",
        "\n",
        "    cleaned_x = self.x_train \n",
        "\n",
        "\n",
        "    x = []\n",
        "    for item in cleaned_x:\n",
        "      cleaned_str = str(\" \".join(item.split(\"\\n\")))\n",
        "      cleaned_str = cleaned_str.replace('.', ' ') \n",
        "      cleaned_str = cleaned_str.replace(',', ' ') \n",
        "      cleaned_str = cleaned_str.replace(':', ' ') \n",
        "      cleaned_str = cleaned_str.replace(';', ' ') \n",
        "      cleaned_str = \" \".join(cleaned_str.split())\n",
        "      cleaned_str=cleaned_str.lower()\n",
        "      word_tokens = cleaned_str.split() \n",
        "      filtered_arr = [w for w in word_tokens if not w in stop_words] \n",
        "      cleaned_str = ' '.join(filtered_arr)\n",
        "      cleaned_str = re.sub(r'[^\\w\\s]','',cleaned_str)\n",
        "\n",
        "      cleaned_str = re.sub(r'\\d+', '', cleaned_str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      portstemmer = PorterStemmer()\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      #lanstemmer = LancasterStemmer()\n",
        "\n",
        "\n",
        "      #Stem\n",
        "      filtered_arr = cleaned_str.split() \n",
        "      stemmed_arr = list([portstemmer.stem(x) for x in filtered_arr])\n",
        "      counts = Counter(stemmed_arr)\n",
        "      repet_arr = [elem for elem in counts if counts[elem]>=1]\n",
        "\n",
        "\n",
        "      #stemmed2_arr = list(set([lanstemmer.stem(x) for x in filtered_arr]))\n",
        "      #lemm_arr = list(set([lemmatizer.lemmatize(x) for x in filtered_arr]))\n",
        "\n",
        "      filtered_sentence = ' '.join(repet_arr)\n",
        "\n",
        "      x.append(filtered_sentence)\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "    print(type(x))\n",
        "    self.x_train = x\n",
        "    \n",
        "    \n",
        "  def preprocessing_data_xtest(self):\n",
        "    stop_words =['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
        "                'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
        "                'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
        "                \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', \n",
        "                'they', 'them', 'their', 'theirs', 'themselves', 'what', \n",
        "                'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these',\n",
        "                'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', \n",
        "                'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
        "                'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
        "                'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', \n",
        "                'with', 'about', 'against', 'between', 'into', 'through', \n",
        "                'during', 'before', 'after', 'above', 'below', 'to', 'from',\n",
        "                'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', \n",
        "                'again', 'further', 'then', 'once', 'here', 'there', 'when', \n",
        "                'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few',\n",
        "                'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', \n",
        "                'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't',\n",
        "                'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", \n",
        "                'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren',\n",
        "                \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn',\n",
        "                \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven',\n",
        "                \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\",\n",
        "                'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", \n",
        "                'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\",\n",
        "                'won', \"won't\", 'wouldn', \"wouldn't\",\"i've\",\"they've\",\"would\",\n",
        "               \"one\",\"two\",\"three\",\"four\",\"five\",\"six\",\"seven\",\"eight\",\"nine\",\"ten\"]\n",
        "\n",
        "    cleaned_x = self.x_test \n",
        "\n",
        "\n",
        "    x = []\n",
        "    for item in cleaned_x:\n",
        "      cleaned_str = str(\" \".join(item.split(\"\\n\")))\n",
        "      cleaned_str = cleaned_str.replace('.', ' ') \n",
        "      cleaned_str = cleaned_str.replace(',', ' ') \n",
        "      cleaned_str = cleaned_str.replace(':', ' ') \n",
        "      cleaned_str = cleaned_str.replace(';', ' ') \n",
        "      cleaned_str = \" \".join(cleaned_str.split())\n",
        "      cleaned_str=cleaned_str.lower()\n",
        "      word_tokens = cleaned_str.split() \n",
        "      filtered_arr = [w for w in word_tokens if not w in stop_words] \n",
        "      cleaned_str = ' '.join(filtered_arr)\n",
        "      cleaned_str = re.sub(r'[^\\w\\s]','',cleaned_str)\n",
        "\n",
        "      cleaned_str = re.sub(r'\\d+', '', cleaned_str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      portstemmer = PorterStemmer()\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "      #lanstemmer = LancasterStemmer()\n",
        "\n",
        "\n",
        "      #Stem\n",
        "      filtered_arr = cleaned_str.split() \n",
        "      stemmed_arr = list([portstemmer.stem(x) for x in filtered_arr])\n",
        "      counts = Counter(stemmed_arr)\n",
        "      repet_arr = [elem for elem in counts if counts[elem]>=1]\n",
        "\n",
        "\n",
        "      #stemmed2_arr = list(set([lanstemmer.stem(x) for x in filtered_arr]))\n",
        "      #lemm_arr = list(set([lemmatizer.lemmatize(x) for x in filtered_arr]))\n",
        "\n",
        "      filtered_sentence = ' '.join(repet_arr)\n",
        "\n",
        "      x.append(filtered_sentence)\n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    print(type(x))  \n",
        "    self.x_test = x\n",
        "    \n",
        "  def ensemble_method(self):\n",
        "    \n",
        "    clf1 = Pipeline([('vect', TfidfVectorizer()),              \n",
        "                  ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
        "                             random_state=1)),\n",
        "                  ])\n",
        "\n",
        "    clf2 = Pipeline([('vect', TfidfVectorizer()),\n",
        "                             ('clf', SGDClassifier(loss='modified_huber', penalty='l2',alpha=0.0001, random_state=42, tol=None)),])\n",
        "\n",
        "\n",
        "    clf3 = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', MultinomialNB(alpha= 0.25))])\n",
        "\n",
        "    classifier = VotingClassifier(estimators=[\n",
        "            ('lr', clf1), ('svm', clf2), ('nb', clf3)],\n",
        "            voting='soft', weights=[1,1,2],\n",
        "            flatten_transform=True)\n",
        "    \n",
        "    classifier = classifier.fit(self.x_train, self.y_train)\n",
        "    \n",
        "    return classifier.predict(self.x_test)  \n",
        "    \n",
        "    \n",
        "  def fit_predict_data(self,modelname):\n",
        "    \n",
        "    if modelname==\"reg\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                    \n",
        "                    ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
        "                             random_state=1)),\n",
        "                    ])\n",
        "    elif modelname == \"nb\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', MultinomialNB(alpha= 0.25))])\n",
        "\n",
        "    elif modelname == \"knn\":\n",
        "      classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                             ('clf', KNeighborsClassifier(leaf_size=100, metric = 'euclidean',n_neighbors=200))])\n",
        "\n",
        "    elif modelname==\"dtree\":\n",
        "      classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                             ('clf', DecisionTreeClassifier(criterion='gini',min_samples_leaf=1, min_samples_split=600))])\n",
        "\n",
        "    elif modelname == \"rforest\":\n",
        "      classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                             ('clf', RandomForestClassifier(n_estimators=10000, max_depth=2, random_state=0))])\n",
        "\n",
        "    elif modelname == \"svm\":\n",
        "      classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                             ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=0.0001, random_state=42, tol=None))])    \n",
        "\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "    classifier.fit(self.x_train, self.y_train)\n",
        "    result = classifier.predict(self.x_test)\n",
        "    return result\n",
        "  \n",
        "  \n",
        " \n",
        "  \n",
        "  def cross_validation_ensemble(self):\n",
        "    foldSize = int(len(self.x_train)/5)\n",
        "    accuracy = []\n",
        "    for i in range (5):\n",
        "      xx_test=self.x_train[i*foldSize:(i+1)*foldSize]\n",
        "      yy_test=self.y_train[i*foldSize:(i+1)*foldSize]\n",
        "      xx_train=np.delete(self.x_train,slice(i*foldSize,(i+1)*foldSize),0)\n",
        "      yy_train=np.delete(self.y_train,slice(i*foldSize,(i+1)*foldSize),0)\n",
        "      \n",
        "      \n",
        "      \n",
        "      #classifier = Pipeline([('vect', TfidfVectorizer(analyzer='word',sublinear_tf=True, min_df=1, norm= \"l2\", strip_accents ='ascii',use_idf=True, ngram_range = (1,1))),\n",
        "                    #('clf', GradientBoostingClassifier(n_estimators=500, learning_rate=1.0,\n",
        "       # max_depth=1, random_state=0)),\n",
        "       #             ])\n",
        "        \n",
        "      clf1 = Pipeline([('vect', TfidfVectorizer()),              \n",
        "                    ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
        "                               random_state=1)),\n",
        "                    ])\n",
        "    \n",
        "      clf2 = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', SGDClassifier(loss='modified_huber', penalty='l2',alpha=0.0001, random_state=42, tol=None)),])\n",
        "\n",
        "\n",
        "      clf3 = Pipeline([('vect', TfidfVectorizer()),\n",
        "                                 ('clf', MultinomialNB(alpha= 0.25))])\n",
        "\n",
        "      classifier = VotingClassifier(estimators=[\n",
        "              ('lr', clf1), ('svm', clf2), ('gnb', clf3)],\n",
        "              voting='soft', weights=[1,1,2],\n",
        "              flatten_transform=True)\n",
        "      \n",
        "      classifier.fit(xx_train, yy_train) \n",
        "      predicted = classifier.predict(xx_test)\n",
        "      acc = np.mean(predicted == yy_test)\n",
        "      accuracy.append(acc)\n",
        "      print(acc)\n",
        "    return np.mean(accuracy)\n",
        "    \n",
        "\n",
        "  \n",
        "  \n",
        "  def cross_validation(self,modelname):\n",
        "    foldSize = int(len(self.x_train)/5)\n",
        "    accuracy = []\n",
        "    for i in range (5):\n",
        "      xx_test=self.x_train[i*foldSize:(i+1)*foldSize]\n",
        "      yy_test=self.y_train[i*foldSize:(i+1)*foldSize]\n",
        "      xx_train=np.delete(self.x_train,slice(i*foldSize,(i+1)*foldSize),0)\n",
        "      yy_train=np.delete(self.y_train,slice(i*foldSize,(i+1)*foldSize),0)\n",
        "      \n",
        "      if modelname==\"reg\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                    ('clf', LogisticRegression(solver='lbfgs', multi_class='multinomial',\n",
        "                               random_state=1)),\n",
        "                    ])\n",
        "      elif modelname == \"nb\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', MultinomialNB(alpha= 0.25))])\n",
        "\n",
        "      elif modelname == \"knn\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', KNeighborsClassifier(leaf_size=100, metric = 'euclidean',n_neighbors=200, n_jobs = 1))])\n",
        "\n",
        "      elif modelname==\"dtree\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', DecisionTreeClassifier(criterion='gini',min_samples_leaf=1, min_samples_split=600))])\n",
        "\n",
        "      elif modelname == \"rforest\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', RandomForestClassifier(n_estimators=10000, max_depth=2, random_state=0))])\n",
        "\n",
        "      elif modelname == \"svm\":\n",
        "        classifier = Pipeline([('vect', TfidfVectorizer()),\n",
        "                               ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=0.0001, random_state=42, tol=None))])    \n",
        "\n",
        "     \n",
        "      classifier.fit(xx_train, yy_train) \n",
        "      predicted = classifier.predict(xx_test)\n",
        "      acc = np.mean(predicted == yy_test)\n",
        "      accuracy.append(acc)\n",
        "      #print(acc)\n",
        "    return np.mean(accuracy)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hevcUM599wqc",
        "colab_type": "text"
      },
      "source": [
        "**Reading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMVd_j4ggds1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "data_train = np.asarray(df_train)\n",
        "x_train = data_train[:,1]\n",
        "y_train = data_train[:,2]\n",
        "\n",
        "data_test = np.asarray(df_test)\n",
        "x_test = data_test[:,1]\n",
        "#y_test = data_test[:,2]\n",
        "print(np.shape(x_train))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90uPGex86nyy",
        "colab_type": "text"
      },
      "source": [
        "**Model running without using Ensemble methods**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3nCLcEY6Q7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models(x_train,y_train,x_test)\n",
        "model.preprocessing_data_xtrain()\n",
        "model.preprocessing_data_xtest()\n",
        "y_result = model.fit_predict_data('nb')\n",
        "#y_result = model.ensemble_method()\n",
        "#print(np.mean(y_result==y_train))\n",
        "#print(y_result)]\n",
        "#print(model.cross_validation_gb())\n",
        "\n",
        "\n",
        "print(model.cross_validation('nb'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmgK2XesCwxF",
        "colab_type": "text"
      },
      "source": [
        "**Models using Ensemble method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF9XMPcA9l9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = models(x_train,y_train,x_test)\n",
        "model.preprocessing_data_xtrain()\n",
        "model.preprocessing_data_xtest()\n",
        "#y_result = model.fit_predict_data('nb')\n",
        "y_result = model.ensemble_method()\n",
        "#print(np.mean(y_result==y_train))\n",
        "\n",
        "print(model.cross_validation_ensemble())\n",
        "\n",
        "\n",
        "#print(model.cross_validation('reg'))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNA8EqwVWgNI",
        "colab_type": "text"
      },
      "source": [
        "**Saving on CSV file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46bJEfIlsWW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission_data = pd.DataFrame(columns=['Id', 'Category'])\n",
        "\n",
        "print(np.shape(data_test[:,0]))\n",
        "print(np.shape(y_result))\n",
        "submission_data['Id'] = data_test[:,0]\n",
        "submission_data['Category'] = y_result\n",
        "submission_data.to_csv(r'/content/drive/My Drive/reddit/result_esm2.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}